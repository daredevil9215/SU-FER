{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2d_clf_problem(X, y, h=None):\n",
    "    '''\n",
    "    Plots a two-dimensional labeled dataset (X,y) and, if function h(x) is given, \n",
    "    the decision surfaces.\n",
    "    '''\n",
    "    assert X.shape[1] == 2, \"Dataset is not two-dimensional\"\n",
    "    if h!=None : \n",
    "        # Create a mesh to plot in\n",
    "        r = 0.03  # mesh resolution\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, r),\n",
    "                             np.arange(y_min, y_max, r))\n",
    "        XX=np.c_[xx.ravel(), yy.ravel()]\n",
    "        try:\n",
    "            Z_test = h(XX)\n",
    "            if Z_test.shape == ():\n",
    "                # h returns a scalar when applied to a matrix; map explicitly\n",
    "                Z = np.array(list(map(h,XX)))\n",
    "            else :\n",
    "                Z = Z_test\n",
    "        except ValueError:\n",
    "            # can't apply to a matrix; map explicitly\n",
    "            Z = np.array(list(map(h,XX)))\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "    # Plot the dataset\n",
    "    plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.tab20b, marker='o', s=50);\n",
    "    \n",
    "def knn_eval(n_instances=100, n_features=2, n_classes=2, n_informative=2, \n",
    "             test_size=0.3, k_range=(1, 20), n_trials=40):\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    ks = list(range(k_range[0], k_range[1] + 1))\n",
    "\n",
    "    for i in range(0, n_trials):\n",
    "        X, y = make_classification(n_instances, n_features, n_classes=n_classes, \n",
    "                                   n_informative=n_informative, n_redundant=0, n_clusters_per_class=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        train = []\n",
    "        test = []\n",
    "        for k in ks:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(X_train, y_train)\n",
    "            train.append(1 - knn.score(X_train, y_train))\n",
    "            test.append(1 - knn.score(X_test, y_test))\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "        \n",
    "    train_errors = np.mean(np.array(train_errors), axis=0)\n",
    "    test_errors = np.mean(np.array(test_errors), axis=0)\n",
    "    best_k = ks[np.argmin(test_errors)]\n",
    "    \n",
    "    return ks, best_k, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Algoritam k-najbližih susjeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ovom zadatku promatrat ćemo jednostavan klasifikacijski model imena **algoritam k-najbližih susjeda**. Najprije ćete ga samostalno isprogramirati kako biste se detaljno upoznali s radom ovog modela, a zatim ćete prijeći na analizu njegovih hiperparametara (koristeći ugrađeni razred, radi efikasnosti)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementirajte klasu `KNN`, koja implementira algoritam $k$ najbližih susjeda. Neobavezan parametar konstruktora jest broj susjeda `n_neighbours` ($k$), čija je podrazumijevana vrijednost 3. Definirajte metode `fit(X, y)` i `predict(X)`, koje služe za učenje modela odnosno predikciju. Kao mjeru udaljenosti koristite euklidsku udaljenost ([`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html); pripazite na parametar `axis`). Nije potrebno implementirati nikakvu težinsku funkciju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, n_neighbors=3):\n",
    "        self.n_neighbors = n_neighbors\n",
    "                \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        self.classes = len(set(self.y))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        rez = []\n",
    "\n",
    "        if len(X_test.shape) == 1:\n",
    "            X_test = X_test.reshape(1, -1)\n",
    "\n",
    "        for x_test in X_test:\n",
    "            distances = []\n",
    "\n",
    "            for i in range(self.X.shape[0]):\n",
    "                distances.append((norm(self.X[i] - x_test), self.y[i]))\n",
    "\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            distances = distances[:self.n_neighbors]\n",
    "            izlaz = [0 for i in range(self.classes)]\n",
    "\n",
    "            for i in range(len(izlaz)):\n",
    "\n",
    "                for j in distances:\n",
    "\n",
    "                    if i == j[1]:\n",
    "                        izlaz[i] += 1\n",
    "\n",
    "            rez.append(np.argmax(np.array(izlaz)))\n",
    "\n",
    "        return np.array(rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako biste se uvjerili da je Vaša implementacija ispravna, usporedite ju s onom u razredu [`neighbors.KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Budući da spomenuti razred koristi razne optimizacijske trikove pri pronalasku najboljih susjeda, obavezno postavite parametar `algorithm=brute`, jer bi se u protivnom moglo dogoditi da vam se predikcije razlikuju. Usporedite modele na danom (umjetnom) skupu podataka (prisjetite se kako se uspoređuju polja; [`numpy.all`](https://numpy.org/doc/stable/reference/generated/numpy.all.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_art, y_art = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                                   n_redundant=0, n_clusters_per_class=2,\n",
    "                                   random_state=np.random.randint(0 ,500))\n",
    "plot_2d_clf_problem(X_art, y_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_art, y_art)\n",
    "\n",
    "izlaz = model.predict(X_art)\n",
    "print(f'Acc: {np.mean(izlaz == y_art)}')\n",
    "\n",
    "model2 = KNeighborsClassifier(algorithm=\"brute\", n_neighbors=3)\n",
    "model2.fit(X_art, y_art)\n",
    "\n",
    "izlaz2 = model2.predict(X_art)\n",
    "print(f'Acc: {np.mean(izlaz2 == y_art)}')\n",
    "\n",
    "\"\"\"\n",
    "dataset = np.array([[2.7810836,2.550537003,0],\n",
    "                    [1.465489372,2.362125076,0],\n",
    "                    [3.396561688,4.400293529,0],\n",
    "                    [1.38807019,1.850220317,0],\n",
    "                    [3.06407232,3.005305973,0],\n",
    "                    [7.627531214,2.759262235,1],\n",
    "                    [5.332441248,2.088626775,1],\n",
    "                    [6.922596716,1.77106367,1],\n",
    "                    [8.675418651,-0.242068655,1],\n",
    "                    [7.673756466,3.508563011,1]])\n",
    "\n",
    "X = dataset[:, :2]\n",
    "y = dataset[:, 2]\n",
    "\n",
    "model.fit(X, y)\n",
    "prediction = model.predict(X)\n",
    "print(f'Expected {y}, Got {prediction}, acc {np.mean(prediction == y)}')\n",
    "\"\"\"\n",
    "\n",
    "plot_2d_clf_problem(X_art, y_art, model.predict)\n",
    "plt.show()\n",
    "\n",
    "plot_2d_clf_problem(X_art, y_art, model2.predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Utjecaj hiperparametra *k*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritam k-nn ima hiperparametar $k$ (broj susjeda). Taj hiperparametar izravno utječe na složenost algoritma, pa je stoga izrazito važno dobro odabrati njegovu vrijednost. Kao i kod mnogih drugih algoritama, tako i kod algoritma k-nn optimalna vrijednost hiperametra $k$ ovisi o konkretnom problemu, uključivo broju primjera $N$, broju značajki (dimenzija) $n$ te broju klasa $K$. \n",
    "\n",
    "Kako bismo dobili pouzdanije rezultate, potrebno je neke od eksperimenata ponoviti na različitim skupovima podataka i zatim uprosječiti dobivene vrijednosti pogrešaka. Koristite funkciju: `knn_eval` koja trenira i ispituje model k-najbližih susjeda na ukupno `n_instances` primjera, i to tako da za svaku vrijednost hiperparametra iz zadanog intervala `k_range` ponovi `n_trials` mjerenja, generirajući za svako od njih nov skup podataka i dijeleći ga na skup za učenje i skup za ispitivanje. Udio skupa za ispitivanje definiran je parametrom `test_size`. Povratna vrijednost funkcije jest četvorka `(ks, best_k, train_errors, test_errors)`. Vrijednost `best_k` je optimalna vrijednost hiperparametra $k$ (vrijednost za koju je pogreška na skupu za ispitivanje najmanja). Vrijednosti `train_errors` i `test_errors`  liste su pogrešaka na skupu za učenja odnosno skupu za testiranje za sve razmatrane vrijednosti hiperparametra $k$, dok `ks` upravo pohranjuje sve razmatrane vrijednosti hiperparametra $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Na podatcima iz zadatka 1, pomoću funkcije `plot_2d_clf_problem` iscrtajte prostor primjera i područja koja odgovaraju prvoj odnosno drugoj klasi. Ponovite ovo za $k\\in[1, 5, 20, 100]$. \n",
    "\n",
    "**NB:** Implementacija algoritma `KNeighborsClassifier` iz paketa `scikit-learn` vjerojatno će raditi brže od Vaše implementacije, pa u preostalim eksperimentima koristite nju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                                   n_redundant=0, n_clusters_per_class=2,\n",
    "                                   random_state=np.random.randint(0, 500))\n",
    "\n",
    "neighbors = [1, 5, 20, 100]\n",
    "\n",
    "for k in neighbors:\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    plot_2d_clf_problem(X, y, model.predict)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako $k$ utječe na izgled granice između klasa?  \n",
    "**Q:** Kako se algoritam ponaša u ekstremnim situacijama: $k=1$ i $k=100$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomoću funkcije `knn_eval`, iscrtajte pogreške učenja i ispitivanja kao funkcije hiperparametra $k\\in\\{1,\\dots,20\\}$, za $N=\\{100, 250, 750\\}$ primjera. Načinite 3 zasebna grafikona. Za svaki ispišite optimalnu vrijednost hiperparametra $k$ (najlakše kao naslov grafikona; vidi [`plt.title`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.title.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = [100, 250, 750]\n",
    "k = np.array([x for x in range(1, 21)])\n",
    "\n",
    "for n in N:\n",
    "    \n",
    "    a, b, c, d = knn_eval(n_instances=n)\n",
    "    plt.plot(k, c, label=\"train\")\n",
    "    plt.plot(k, d, label=\"test\")\n",
    "    plt.title(label=f'Best k: {b}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako se mijenja optimalna vrijednost hiperparametra $k$ s obzirom na broj primjera $N$? Zašto?  \n",
    "**Q:** Kojem području odgovara prenaučenost, a kojem podnaučenost modela? Zašto?  \n",
    "**Q:** Je li uvijek moguće doseći pogrešku od 0 na skupu za učenje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nebitne značajke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako bismo provjerili u kojoj je mjeri algoritam k-najbližih susjeda osjetljiv na prisustvo nebitnih značajki, možemo iskoristiti funkciju [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) kako bismo generirali skup primjera kojemu su neke od značajki nebitne. Naime, parametar `n_informative` određuje broj bitnih značajki, dok parametar `n_features` određuje ukupan broj značajki. Ako je `n_features > n_informative`, onda će neke od značajki biti nebitne. Umjesto da izravno upotrijebimo funkciju `make_classification`, upotrijebit ćemo funkciju `knn_eval`, koja samo preuzime ove parametre, ali nam omogućuje pouzdanije procjene.\n",
    "\n",
    "Koristite funkciju `mlutils.knn_eval` na dva načina. U oba koristite $N=1000$ primjera, $n=10$ značajki i $K=5$ klasa, ali za prvi neka su svih 10 značajki bitne, a za drugi neka je bitno samo 5 od 10 značajki. Ispišite pogreške učenja i ispitivanja za oba modela za optimalnu vrijednost $k$ (vrijednost za koju je ispitna pogreška najmanja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [10, 5]:\n",
    "\n",
    "    a, b, c, d = knn_eval(n_instances=1000, n_classes=5, n_features=10, n_informative=i)\n",
    "    print(f'Best train: {c[b - 1]}, best test: {d[b - 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Je li algoritam k-najbližih susjeda osjetljiv na nebitne značajke? Zašto?  \n",
    "**Q:** Je li ovaj problem izražen i kod ostalih modela koje smo dosad radili (npr. logistička regresija)?  \n",
    "**Q:** Kako bi se model k-najbližih susjeda ponašao na skupu podataka sa značajkama različitih skala? Detaljno pojasnite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. \"Prokletstvo dimenzionalnosti\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Prokletstvo dimenzionalnosti\" zbirni je naziv za niz fenomena povezanih s visokodimenzijskim prostorima. Ti fenomeni, koji se uglavnom protive našoj intuiciji, u većini slučajeva dovode do toga da se s porastom broja dimenzija (značajki) smanjenje točnost modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Općenito, povećanje dimenzija dovodi do toga da sve točke u ulaznome prostoru postaju (u smislu euklidske udaljenosti) sve udaljenije jedne od drugih te se, posljedično, gube razlike u udaljenostima između točaka. Eksperimentalno ćemo provjeriti da je to doista slučaj. Proučite funkciju [`metrics.pairwise_distances`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). Generirajte 100 slučajnih vektora u različitim dimenzijama $n\\in[1,2,\\ldots,50]$ dimenzija te izračunajte *prosječnu* euklidsku udaljenost između svih parova tih vektora. Za generiranje slučajnih vektora koristite funkciju [`numpy.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html). Na istom grafu skicirajte i krivulju za prosječne kosinusne udaljenosti (parametar `metric`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "dims = range(1, 51)\n",
    "mean_euclid_dists = list()\n",
    "mean_cosine_dists = list()\n",
    "\n",
    "for n in dims:\n",
    "    X = [np.random.random(size=n) for i in range(100)]\n",
    "    mean_euclid_dists.append(np.mean(pairwise_distances(X)))\n",
    "    mean_cosine_dists.append(np.mean(pairwise_distances(X, metric='cosine')))\n",
    "    \n",
    "plt.plot(dims, mean_euclid_dists)\n",
    "plt.plot(dims, mean_cosine_dists)\n",
    "plt.legend(['euclid', 'cosine'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Pokušajte objasniti razlike u rezultatima. Koju biste od ovih dviju mjera koristili za klasifikaciju visokodimenzijskih podataka?  \n",
    "**Q:** Zašto je ovaj problem osobito izražen kod algoritma k-najbližih susjeda?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
