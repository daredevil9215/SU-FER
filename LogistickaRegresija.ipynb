{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2d_clf_problem(X, y, h=None):\n",
    "    '''\n",
    "    Plots a two-dimensional labeled dataset (X,y) and, if function h(x) is given, \n",
    "    the decision surfaces.\n",
    "    '''\n",
    "    assert X.shape[1] == 2, \"Dataset is not two-dimensional\"\n",
    "    if h!=None : \n",
    "        # Create a mesh to plot in\n",
    "        r = 0.04  # mesh resolution\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, r),\n",
    "                             np.arange(y_min, y_max, r))\n",
    "        XX=np.c_[xx.ravel(), yy.ravel()]\n",
    "        try:\n",
    "            Z_test = h(XX)\n",
    "            if Z_test.shape == ():\n",
    "                # h returns a scalar when applied to a matrix; map explicitly\n",
    "                Z = np.array(list(map(h,XX)))\n",
    "            else :\n",
    "                Z = Z_test\n",
    "        except ValueError:\n",
    "            # can't apply to a matrix; map explicitly\n",
    "            Z = np.array(list(map(h,XX)))\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "    # Plot the dataset\n",
    "    plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.tab20b, marker='o', s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistička regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovaj zadatak bavi se probabilističkim diskriminativnim modelom, **logističkom regresijom**, koja je, unatoč nazivu, klasifikacijski model.\n",
    "\n",
    "Logistička regresija tipičan je predstavnik tzv. **poopćenih linearnih modela** koji su oblika: $h(\\mathbf{x})=f(\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}})$. Logistička funkcija za funkciju $f$ koristi tzv. **logističku** (sigmoidalnu) funkciju $\\sigma (x) = \\frac{1}{1 + \\textit{exp}(-x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)  \n",
    "\n",
    "Definirajte logističku (sigmoidalnu) funkciju $\\mathrm{sigm}(x)=\\frac{1}{1+\\exp(-\\alpha x)}$ i prikažite je za $\\alpha\\in\\{1,2,4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigm(x, a):\n",
    "    return 1 / (1 + np.exp(-a * x))\n",
    "\n",
    "X = np.linspace(-5, 5)\n",
    "\n",
    "for i in [1, 2, 4]:\n",
    "    y = sigm(X, i)\n",
    "    plt.plot(X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Zašto je sigmoidalna funkcija prikladan izbor za aktivacijsku funkciju poopćenoga linearnog modela? \n",
    "</br>\n",
    "\n",
    "**Q**: Kakav utjecaj ima faktor $\\alpha$ na oblik sigmoide? Što to znači za model logističke regresije (tj. kako izlaz modela ovisi o normi vektora težina $\\mathbf{w}$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Implementirajte funkciju \n",
    "\n",
    "> `lr_train(X, y, eta=0.01, max_iter=2000, alpha=0, epsilon=0.0001, trace=False)` \n",
    "\n",
    "za treniranje modela logističke regresije gradijentnim spustom (*batch* izvedba). Funkcija uzima označeni skup primjera za učenje (matrica primjera `X` i vektor oznaka `y`) te vraća $(n+1)$-dimenzijski vektor težina tipa `ndarray`. Ako je `trace=True`, funkcija dodatno vraća listu (ili matricu) vektora težina $\\mathbf{w}^0,\\mathbf{w}^1,\\dots,\\mathbf{w}^k$ generiranih kroz sve iteracije optimizacije, od 0 do $k$. Optimizaciju treba provoditi dok se ne dosegne `max_iter` iteracija, ili kada razlika u pogrešci unakrsne entropije između dviju iteracija padne ispod vrijednosti `epsilon`. Parametar `alpha` predstavlja faktor L2-regularizacije.\n",
    "\n",
    "Preporučamo definiranje pomoćne funkcije `lr_h(x,w)` koja daje predikciju za primjer `x` uz zadane težine `w`. Također, preporučamo i funkciju `cross_entropy_error(X,y,w)` koja izračunava pogrešku unakrsne entropije modela na označenom skupu `(X,y)` uz te iste težine.\n",
    "\n",
    "**NB:** Obratite pozornost na to da je način kako su definirane oznake ($\\{+1,-1\\}$ ili $\\{1,0\\}$) kompatibilan s izračunom funkcije gubitka u optimizacijskome algoritmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(1)\n",
    "\n",
    "def lr_h(x, w):\n",
    "    x = np.append(np.ones((x.shape[0], 1)), x, axis=1)\n",
    "    return 1 / (1 + np.exp(-1 * x @ w))\n",
    "\n",
    "def cross_entropy_error(X, y, w):\n",
    "    return np.mean(-y.reshape(-1, 1) * np.log(lr_h(X, w)) - (1 - y.reshape(-1, 1)) * np.log(1 - lr_h(X, w)))\n",
    "\n",
    "def lr_train(X, y, eta=0.01, max_iter=2000, alpha=0, epsilon=0.0001, trace=False):\n",
    "    \n",
    "    weight_matrix = []\n",
    "    last_error = 0\n",
    "    w = np.zeros((X.shape[1] + 1, 1))\n",
    "\n",
    "    for i in range(max_iter + 1):\n",
    "\n",
    "        weight_matrix.append(w.copy())\n",
    "\n",
    "        cur_error = cross_entropy_error(X, y, w)\n",
    "        if np.abs(cur_error - last_error) < epsilon:\n",
    "            break\n",
    "\n",
    "        dw = np.zeros((X.shape[1] + 1, 1))\n",
    "\n",
    "        for j in range(X.shape[0]):\n",
    "\n",
    "            h = lr_h(X[j].reshape(1, -1), w)\n",
    "            dw = dw - (h - y[j].reshape(1, 1)) * np.append(1, X[j]).reshape(-1, 1)\n",
    "\n",
    "        w[0] = w[0] + eta * dw[0]\n",
    "        w[1:] = w[1:] * (1 - eta * alpha) + eta * dw[1:]\n",
    "\n",
    "    weight_matrix = np.array(weight_matrix)\n",
    "\n",
    "    if trace:\n",
    "        return w, weight_matrix\n",
    "    else:\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Koristeći funkciju `lr_train`, trenirajte model logističke regresije na skupu `seven`, prikažite dobivenu granicu između klasa te  izračunajte pogrešku unakrsne entropije. \n",
    "\n",
    "**NB:** Pripazite da modelu date dovoljan broj iteracija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seven_X = np.array([[2,1], \n",
    "                    [2,3], \n",
    "                    [1,2], \n",
    "                    [3,2], \n",
    "                    [5,2], \n",
    "                    [5,4], \n",
    "                    [6,3]])\n",
    "\n",
    "seven_y = np.array([1, 1, 1, 1, 0, 0, 0])\n",
    "\n",
    "w, weights_legacy = lr_train(seven_X, seven_y, trace=True, eta=0.01)\n",
    "\n",
    "print(f'Weights:\\n {w}')\n",
    "print(f'Error: {cross_entropy_error(seven_X, seven_y, w)}')\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(seven_X, seven_y)\n",
    "predict = model.predict(seven_X)\n",
    "print(f'Model predict: {predict}')\n",
    "print(model.coef_, model.intercept_)\n",
    "\n",
    "\n",
    "plot_2d_clf_problem(seven_X, seven_y, lambda x: lr_h(x, w) >= 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji kriterij zaustavljanja je aktiviran?\n",
    "\n",
    "**Q:** Zašto dobivena pogreška unakrsne entropije nije jednaka nuli?\n",
    "\n",
    "**Q:** Kako biste utvrdili da je optimizacijski postupak doista pronašao hipotezu koja minimizira pogrešku učenja? O čemu to ovisi?\n",
    "\n",
    "**Q:** Na koji način biste preinačili kôd ako biste htjeli da se optimizacija izvodi stohastičkim gradijentnim spustom (*online learning*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Prikažite na jednom grafikonu pogrešku unakrsne entropije (očekivanje logističkog gubitka) i pogrešku klasifikacije (očekivanje gubitka 0-1) na skupu `seven` kroz iteracije optimizacijskog postupka. Koristite trag težina funkcije `lr_train` iz zadatka (b) (opcija `trace=True`). Na drugom grafikonu prikažite pogrešku unakrsne entropije kao funkciju broja iteracija za različite stope učenja, $\\eta\\in\\{0.005,0.01,0.05,0.1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "errors_entropy, errors_zero_one = [], []\n",
    "\n",
    "for w in weights_legacy:\n",
    "    errors_entropy.append(cross_entropy_error(seven_X, seven_y, w))\n",
    "    errors_zero_one.append(zero_one_loss(seven_y, lr_h(seven_X, w) >= 0.5))\n",
    "\n",
    "errors_entropy = np.array(errors_entropy)\n",
    "errors_zero_one = np.array(errors_zero_one)\n",
    "x_axis = np.array([x for x in range(2001)])\n",
    "\n",
    "plt.plot(x_axis, errors_entropy)\n",
    "plt.plot(x_axis, errors_zero_one)\n",
    "plt.show()\n",
    "\n",
    "num_of_iter = 2000\n",
    "for lr in [0.005, 0.01, 0.05, 0.1]: #0.01, 0.05, 0.1]:\n",
    "    w, weights_legacy = lr_train(seven_X, seven_y, trace=True, eta=lr, max_iter=num_of_iter)\n",
    "\n",
    "    errors_entropy = []\n",
    "    for w in weights_legacy:\n",
    "        errors_entropy.append(cross_entropy_error(seven_X, seven_y, w))\n",
    "    errors_entropy = np.array(errors_entropy)\n",
    "    x_axis = np.array([x for x in range(num_of_iter + 1)])\n",
    "    plt.plot(x_axis, errors_entropy, label=lr)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q:** Zašto je pogreška unakrsne entropije veća od pogreške klasifikacije? Je li to uvijek slučaj kod logističke regresije i zašto?\n",
    "\n",
    "**Q:** Koju stopu učenja $\\eta$ biste odabrali i zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Upoznajte se s klasom [`linear_model.LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) koja implementira logističku regresiju. Usporedite rezultat modela na skupu `seven` s rezultatom koji dobivate pomoću vlastite implementacije algoritma.\n",
    "\n",
    "**NB:** Kako ugrađena implementacija koristi naprednije verzije optimizacije funkcije, vrlo je vjerojatno da Vam se rješenja neće poklapati, ali generalne performanse modela bi trebale. Ponovno, pripazite na broj iteracija i snagu regularizacije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(seven_X, seven_y)\n",
    "predict = model.predict(seven_X)\n",
    "\n",
    "plot_2d_clf_problem(seven_X, seven_y, model.predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analiza logističke regresije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Koristeći ugrađenu implementaciju logističke regresije, provjerite kako se logistička regresija nosi s vrijednostima koje odskaču. Iskoristite skup `outlier`. Prikažite granicu između klasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_X = np.append(seven_X, [[12,8]], axis=0)\n",
    "outlier_y = np.append(seven_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(outlier_X, outlier_y)\n",
    "predict = model.predict(outlier_X)\n",
    "\n",
    "plot_2d_clf_problem(outlier_X, outlier_y, model.predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Zašto se rezultat razlikuje od onog koji je dobio model klasifikacije linearnom regresijom iz prvog zadatka?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Trenirajte model logističke regresije na skupu `seven` te na dva odvojena grafikona prikažite, kroz iteracije optimizacijskoga algoritma, (1) izlaz modela $h(\\mathbf{x})$ za svih sedam primjera te (2) vrijednosti težina $w_0$, $w_1$, $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, weights_legacy = lr_train(seven_X, seven_y, trace=True)\n",
    "\n",
    "izlazi = [[] for x in range(7)]\n",
    "tezine = [[] for x in range(3)]\n",
    "\n",
    "for w in weights_legacy:\n",
    "\n",
    "    output = lr_h(seven_X, w)\n",
    "\n",
    "    for i in range(7):\n",
    "        izlazi[i].append(output[i])\n",
    "\n",
    "    for i in range(3):\n",
    "        tezine[i].append(w[i])\n",
    "\n",
    "for a in izlazi:\n",
    "    a = np.array(a)\n",
    "\n",
    "for a in tezine:\n",
    "    a = np.array(a)\n",
    "\n",
    "for i in range(7):\n",
    "    plt.plot(x_axis, izlazi[i], label=f'{seven_X[i]}, {seven_y[i]}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(x_axis, tezine[i], label=f'w{i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Ponovite eksperiment iz podzadatka (b) koristeći linearno neodvojiv skup podataka `unsep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsep_X = np.append(seven_X, [[2,2]], axis=0)\n",
    "unsep_y = np.append(seven_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, weights_legacy = lr_train(unsep_X, unsep_y, trace=True)\n",
    "\n",
    "plot_2d_clf_problem(unsep_X, unsep_y, lambda x: lr_h(x, w) >= 0.5)\n",
    "plt.show()\n",
    "\n",
    "izlazi = [[] for x in range(8)]\n",
    "tezine = [[] for x in range(3)]\n",
    "\n",
    "print(unsep_X.shape)\n",
    "\n",
    "for w in weights_legacy:\n",
    "\n",
    "    output = lr_h(unsep_X, w)\n",
    "\n",
    "    for i in range(8):\n",
    "        izlazi[i].append(output[i])\n",
    "\n",
    "    for i in range(3):\n",
    "        tezine[i].append(w[i])\n",
    "\n",
    "for a in izlazi:\n",
    "    a = np.array(a)\n",
    "\n",
    "for a in tezine:\n",
    "    a = np.array(a)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.plot(x_axis, izlazi[i], label=f'{unsep_X[i]}, {unsep_y[i]}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(x_axis, tezine[i], label=f'w{i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Usporedite grafikone za slučaj linearno odvojivih i linearno neodvojivih primjera te komentirajte razliku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularizirana logistička regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model logističke regresije na skupu `seven` s različitim faktorima L2-regularizacije, $\\alpha\\in\\{0,1,10,100\\}$. Prikažite na dva odvojena grafikona (1) pogrešku unakrsne entropije te (2) L2-normu vektora $\\mathbf{w}$ kroz iteracije optimizacijskog algoritma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "err_glob, norm_glob = [], []\n",
    "\n",
    "for a in [0, 1, 10, 100]:\n",
    "\n",
    "    w, weights_legacy = lr_train(seven_X, seven_y, trace=True, alpha=a)\n",
    "\n",
    "    errors = []\n",
    "    norms = []\n",
    "\n",
    "    for w in weights_legacy:\n",
    "        errors.append(cross_entropy_error(seven_X, seven_y, w))\n",
    "        norms.append(norm(w, ord=2))\n",
    "\n",
    "    errors = np.array(errors)\n",
    "    norms = np.array(norms)\n",
    "\n",
    "    err_glob.append(errors)\n",
    "    norm_glob.append(norms)\n",
    "\n",
    "\n",
    "for i in range(len(err_glob)):\n",
    "    if i == 0:\n",
    "        plt.plot(x_axis, err_glob[i], label=f'a={i}')\n",
    "    else:\n",
    "        plt.plot(x_axis, err_glob[i], label=f'a={10**(i - 1)}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(norm_glob)):\n",
    "    if i == 0:\n",
    "        plt.plot(x_axis, norm_glob[i], label=f'a={i}')\n",
    "    else:\n",
    "        plt.plot(x_axis, norm_glob[i], label=f'a={10**(i - 1)}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Jesu li izgledi krivulja očekivani i zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednost za $\\alpha$ odabrali i zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistička regresija s funkcijom preslikavanja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite funkciju [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html). Generirajte i prikažite dvoklasan skup podataka s ukupno $N=100$ dvodimenzijskih ($n=2)$ primjera, i to sa dvije grupe po klasi (`n_clusters_per_class=2`). Malo je izgledno da će tako generiran skup biti linearno odvojiv, međutim to nije problem jer primjere možemo preslikati u višedimenzijski prostor značajki pomoću klase [`preprocessing.PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), kao što smo to učinili kod linearne regresije u prvoj laboratorijskoj vježbi. Trenirajte model logističke regresije koristeći za preslikavanje u prostor značajki polinomijalnu funkciju stupnja $d=2$ i stupnja $d=3$. Prikažite dobivene granice između klasa. Možete koristiti svoju implementaciju, ali se radi brzine preporuča koristiti `linear_model.LogisticRegression`. Regularizacijski faktor odaberite po želji.\n",
    "\n",
    "**NB:** Kao i ranije, za prikaz granice između klasa koristite funkciju `plot_2d_clf_problem`. Funkciji kao argumente predajte izvorni skup podataka, a preslikavanje u prostor značajki napravite unutar poziva funkcije `h` koja čini predikciju, na sljedeći način:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#plot_2d_clf_problem(X, y, lambda x : model.predict(poly.transform(x))\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_classes=2, n_clusters_per_class=2, n_redundant=0, n_informative=2, n_features=2)\n",
    "\n",
    "plot_2d_clf_problem(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "for d in [2, 3]:\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_extended = poly.fit_transform(X)\n",
    "    model.fit(X_extended, y)\n",
    "    plot_2d_clf_problem(X, y, lambda x: model.predict(poly.fit_transform(x)))\n",
    "    plt.show()\n",
    "    print(X_extended[0])\n",
    "    print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [2, 3]:\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_extended = poly.fit_transform(X)\n",
    "    w = lr_train(X_extended, y)\n",
    "    plot_2d_clf_problem(X, y, lambda x: lr_h(poly.fit_transform(x), w) >= 0.5)\n",
    "    plt.show()\n",
    "    print(cross_entropy_error(X_extended, y, w))\n",
    "    print(X_extended[0])\n",
    "    print(w.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji biste stupanj polinoma upotrijebili i zašto? Je li taj odabir povezan s odabirom regularizacijskog faktora $\\alpha$? Zašto?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
