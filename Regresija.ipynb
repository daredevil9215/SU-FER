{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jednostavna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadan je skup primjera $\\mathcal{D}=\\{(x^{(i)},y^{(i)})\\}_{i=1}^4 = \\{(0,4),(1,1),(2,2),(4,5)\\}$. Primjere predstavite matricom $\\mathbf{X}$ dimenzija $N\\times n$ (u ovom slučaju $4\\times 1$) i vektorom oznaka $\\textbf{y}$, dimenzija $N\\times 1$ (u ovom slučaju $4\\times 1$), na sljedeći način:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Proučite funkciju [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) iz biblioteke `sklearn` i upotrijebite je za generiranje matrice dizajna $\\mathbf{\\Phi}$ koja ne koristi preslikavanje u prostor više dimenzije (samo će svakom primjeru biti dodane *dummy* jedinice; $m=n+1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(1)\n",
    "phi = poly.fit_transform(X)\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s modulom [`linalg`](http://docs.scipy.org/doc/numpy/reference/routines.linalg.html). Izračunajte težine $\\mathbf{w}$ modela linearne regresije kao $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Zatim se uvjerite da isti rezultat možete dobiti izračunom pseudoinverza $\\mathbf{\\Phi}^+$ matrice dizajna, tj. $\\mathbf{w}=\\mathbf{\\Phi}^+\\mathbf{y}$, korištenjem funkcije [`pinv`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "w = np.dot(np.dot(linalg.inv(np.dot(phi.T, phi)), phi.T), y)\n",
    "print(w)\n",
    "w = linalg.pinv(phi) @ y\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radi jasnoće, u nastavku je vektor $\\mathbf{x}$ s dodanom *dummy* jedinicom $x_0=1$ označen kao $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite primjere iz $\\mathcal{D}$ i funkciju $h(\\tilde{\\mathbf{x}})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$. Izračunajte pogrešku učenja prema izrazu $E(h|\\mathcal{D})=\\frac{1}{2}\\sum_{i=1}^N(\\tilde{\\mathbf{y}}^{(i)} - h(\\tilde{\\mathbf{x}}^{(i)}))^2$. Možete koristiti funkciju srednje kvadratne pogreške [`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) iz modula [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "**Q:** Gore definirana funkcija pogreške $E(h|\\mathcal{D})$ i funkcija srednje kvadratne pogreške nisu posve identične. U čemu je razlika? Koja je \"realnija\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#h(x) = w^T x\n",
    "for x in phi:\n",
    "    print(f'h({x}) = {w @ x}')\n",
    "\n",
    "h = phi @ w\n",
    "print(f'h = {h}')\n",
    "\n",
    "error = 0\n",
    "for i in range(4):\n",
    "    error += (y[i] - h[i])**2\n",
    "error *= 0.5\n",
    "\n",
    "print(f'E(h|D) = {error}')\n",
    "\n",
    "print(f'Mean squared error = {mean_squared_error(y, h)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uvjerite se da za primjere iz $\\mathcal{D}$ težine $\\mathbf{w}$ ne možemo naći rješavanjem sustava $\\mathbf{w}=\\mathbf{\\Phi}^{-1}\\mathbf{y}$, već da nam doista treba pseudoinverz.\n",
    "\n",
    "**Q:** Zašto je to slučaj? Bi li se problem mogao riješiti preslikavanjem primjera u višu dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu primjera $\\mathcal{D}$? Pokažite na primjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    w = linalg.inv(phi) @ y\n",
    "except:\n",
    "    print(\"Ne ide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). Provjerite jesu li težine koje izračunava ta funkcija (dostupne pomoću atributa `coef_` i `intercept_`) jednake onima koje ste izračunali gore. Ako nisu, prilagodite kôd tako da jest.\n",
    "\n",
    "**NB:** Obratite pozornost na to kako klase [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) i [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) koriste pomak i osigurajte da ga ne dodajete više puta.\n",
    "\n",
    "Izračunajte predikcije modela (metoda `predict`) i uvjerite se da je pogreška učenja identična onoj koju ste ranije izračunali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rez = LinearRegression().fit(phi, y)\n",
    "print(rez.score(phi, y))\n",
    "print(rez.coef_)\n",
    "print(rez.intercept_)\n",
    "print(rez.predict(np.array([[1, 2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polinomijalna regresija i utjecaj šuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Razmotrimo sada regresiju na većem broju primjera. Definirajte funkciju `make_labels(X, f, noise=0)` koja uzima matricu neoznačenih primjera $\\mathbf{X}_{N\\times n}$ te generira vektor njihovih oznaka $\\mathbf{y}_{N\\times 1}$. Oznake se generiraju kao $y^{(i)} = f(x^{(i)})+\\mathcal{N}(0,\\sigma^2)$, gdje je $f:\\mathbb{R}^n\\to\\mathbb{R}$ stvarna funkcija koja je generirala podatke (koja nam je u stvarnosti nepoznata), a $\\sigma$ je standardna devijacija Gaussovog šuma, definirana parametrom `noise`. Za generiranje šuma možete koristiti funkciju [`numpy.random.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html). \n",
    "\n",
    "Generirajte skup za učenje od $N=50$ primjera uniformno distribuiranih u intervalu $[-5,5]$ pomoću funkcije $f(x) = 5 + x -2 x^2 -5 x^3$ uz šum  $\\sigma=200$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "def make_labels(X, f, noise=0):\n",
    "    rez = []\n",
    "    for i in X:\n",
    "        rez.append(f(i) + normal(0, noise))\n",
    "    return np.array(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_instances(x1, x2, N) :\n",
    "    return np.array([np.array([x]) for x in np.linspace(x1,x2,N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = make_instances(-5, 5, 50)\n",
    "b = make_labels(a, lambda x: 5 + x - 2 * x**2 - 5 * x**3, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite taj skup funkcijom [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(a, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model polinomijalne regresije stupnja $d=3$. Na istom grafikonu prikažite naučeni model $h(\\mathbf{x})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$ i primjere za učenje. Izračunajte pogrešku učenja modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "novi = PolynomialFeatures(3).fit_transform(a.reshape(-1, 1))\n",
    "model.fit(novi, b)\n",
    "predict = model.predict(novi)\n",
    "print(f'Error: {mean_squared_error(b, predict)}')\n",
    "print(model.coef_)\n",
    "plt.plot(a, predict)\n",
    "plt.scatter(a, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Odabir modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Na skupu podataka iz zadatka 2 trenirajte pet modela linearne regresije $\\mathcal{H}_d$ različite složenosti, gdje je $d$ stupanj polinoma, $d\\in\\{1,3,5,10,20\\}$. Prikažite na istome grafikonu skup za učenje i funkcije $h_d(\\mathbf{x})$ za svih pet modela (preporučujemo koristiti `plot` unutar `for` petlje). Izračunajte pogrešku učenja svakog od modela.\n",
    "\n",
    "**Q:** Koji model ima najmanju pogrešku učenja i zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stupanj = [1, 3, 5, 10, 20]\n",
    "for d in stupanj:\n",
    "    novi = PolynomialFeatures(d).fit_transform(a.reshape(-1, 1))\n",
    "    model.fit(novi, b)\n",
    "    predict = model.predict(novi)\n",
    "    print(f'Error stupnja {d}: {mean_squared_error(b, predict)}')\n",
    "    plt.plot(a, predict)\n",
    "plt.scatter(a, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojite skup primjera iz zadatka 2 pomoću funkcije [`model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) na skup za učenja i skup za ispitivanje u omjeru 1:1. Prikažite na jednom grafikonu pogrešku učenja i ispitnu pogrešku za modele polinomijalne regresije $\\mathcal{H}_d$, sa stupnjem polinoma $d$ u rasponu $d\\in [1,2,\\ldots,20]$. Budući da kvadratna pogreška brzo raste za veće stupnjeve polinoma, umjesto da iscrtate izravno iznose pogrešaka, iscrtajte njihove logaritme.\n",
    "\n",
    "**NB:** Podjela na skupa za učenje i skup za ispitivanje mora za svih pet modela biti identična.\n",
    "\n",
    "**Q:** Je li rezultat u skladu s očekivanjima? Koji biste model odabrali i zašto?\n",
    "\n",
    "**Q:** Pokrenite iscrtavanje više puta. U čemu je problem? Bi li problem bio jednako izražen kad bismo imali više primjera? Zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, b, test_size=0.5, random_state=83)\n",
    "stupanj = [x for x in range(1, 21)]\n",
    "greske_train = []\n",
    "greske_test = []\n",
    "\n",
    "for d in stupanj:\n",
    "\n",
    "    novi_train = PolynomialFeatures(d).fit_transform(X_train)\n",
    "    model.fit(novi_train, y_train)\n",
    "    predict = model.predict(novi_train)\n",
    "    greske_train.append(mean_squared_error(y_train, predict))\n",
    "    #if d % 5 == 0:\n",
    "        #print(f'Greska train: {np.log(mean_squared_error(y_train, predict))}')\n",
    "\n",
    "    novi_test = PolynomialFeatures(d).fit_transform(X_test)\n",
    "    predict = model.predict(novi_test)\n",
    "    greske_test.append(mean_squared_error(y_test, predict))\n",
    "    #if d % 5 == 0:\n",
    "        #print(f'Greska test: {np.log(mean_squared_error(y_test, predict))}')\n",
    "\n",
    "#greske = np.log([i[1]/i[0] for i in zip(greske_train, greske_test)]).reshape(-1, 1)\n",
    "#print(greske)\n",
    "\n",
    "plt.plot(np.array(stupanj).reshape(-1, 1), np.log(np.array(greske_train).reshape(-1, 1)))\n",
    "plt.plot(np.array(stupanj).reshape(-1, 1), np.log(np.array(greske_test).reshape(-1, 1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Točnost modela ovisi o (1) njegovoj složenosti (stupanj $d$ polinoma), (2) broju primjera $N$, i (3) količini šuma. Kako biste to analizirali, nacrtajte grafikone pogrešaka kao u 3b, ali za različit $N\\in$ (trećina, dvije trećine, sve) i količine šuma $\\sigma\\in\\{100,200,500\\}$ (ukupno 9 grafikona). Upotrijebite funkciju [`subplots`](http://matplotlib.org/examples/pylab_examples/subplots_demo.html) kako biste pregledno posložili grafikone u tablicu $3\\times 3$. Podatci se generiraju na isti način kao u zadatku 2.\n",
    "\n",
    "**NB:** Pobrinite se da svi grafikoni budu generirani nad usporedivim skupovima podataka, na sljedeći način. Generirajte najprije svih 1000 primjera, podijelite ih na skupove za učenje i skupove za ispitivanje (dva skupa od po 500 primjera). Zatim i od skupa za učenje i od skupa za ispitivanje načinite tri različite verzije, svaka s drugačijom količinom šuma (ukupno 2x3=6 verzija podataka). Kako bi simulirali veličinu skupa podataka, od tih dobivenih 6 skupova podataka uzorkujte trećinu, dvije trećine i sve podatke. Time ste dobili 18 skupova podataka -- skup za učenje i za testiranje za svaki od devet grafova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = make_instances(-5, 5, 1000)\n",
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=1745)\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for noise in [100, 200, 500]:\n",
    "    y_train.append(make_labels(X_train, lambda x: 5 + x - 2 * x**2 - 5 * x**3, noise))\n",
    "    y_test.append(make_labels(X_test, lambda x: 5 + x - 2 * x**2 - 5 * x**3, noise))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "data_train = []\n",
    "data_test = []\n",
    "\n",
    "for i in [1/3, 2/3, 1]:\n",
    "    \n",
    "    for y in y_train:\n",
    "\n",
    "        data_train.append((X_train[:int(i * len(X_train))], y[:int(i * len(X_train))]))\n",
    "\n",
    "    for y in y_test:\n",
    "\n",
    "        data_test.append((X_test[:int(i * len(X_test))], y[:int(i * len(X_test))]))\n",
    "\n",
    "errors = []\n",
    "\n",
    "for ((x_train, y_train), (x_test, y_test)) in zip(data_train, data_test): #((x_train, y_train), (x_test, y_test))\n",
    "\n",
    "    errors_train = []\n",
    "    errors_test = []\n",
    "    \n",
    "    for d in stupanj:\n",
    "\n",
    "        novi_train = PolynomialFeatures(d).fit_transform(x_train)\n",
    "        model.fit(novi_train, y_train)\n",
    "        predict_train = model.predict(novi_train)\n",
    "        errors_train.append(mean_squared_error(y_train, predict_train))\n",
    "\n",
    "        novi_test = PolynomialFeatures(d).fit_transform(x_test)\n",
    "        predict_test = model.predict(novi_test)\n",
    "        errors_test.append(mean_squared_error(y_test, predict_test))\n",
    "\n",
    "    errors_train = np.log(np.array(errors_train))\n",
    "    errors_test = np.log(np.array(errors_test))\n",
    "\n",
    "    errors.append((errors_train, errors_test, (len(x_train))))\n",
    "\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in errors:\n",
    "\n",
    "    axarr[a, b].plot(stupanj, i[0])\n",
    "    axarr[a, b].plot(stupanj, i[1])\n",
    "    axarr[a, b].set_title(f'N: {i[2]}')\n",
    "\n",
    "    b += 1\n",
    "\n",
    "    if b == 3:\n",
    "        a += 1\n",
    "        b = 0\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "axarr[0, 0].plot(x, y)\n",
    "axarr[0, 0].set_title('Axis [0,0]')\n",
    "axarr[0, 1].scatter(x, y)\n",
    "axarr[0, 1].set_title('Axis [0,1]')\n",
    "axarr[1, 0].plot(x, y ** 2)\n",
    "axarr[1, 0].set_title('Axis [1,0]')\n",
    "axarr[1, 1].scatter(x, y ** 2)\n",
    "axarr[1, 1].set_title('Axis [1,1]')\n",
    "# Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q:*** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularizirana regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "U gornjim eksperimentima nismo koristili **regularizaciju**. Vratimo se najprije na primjer iz zadatka 1. Na primjerima iz tog zadatka izračunajte težine $\\mathbf{w}$ za polinomijalni regresijski model stupnja $d=3$ uz L2-regularizaciju (tzv. *ridge regression*), prema izrazu $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Napravite izračun težina za regularizacijske faktore $\\lambda=0$, $\\lambda=1$ i $\\lambda=10$ te usporedite dobivene težine.\n",
    "\n",
    "**Q:** Kojih je dimenzija matrica koju treba invertirati?\n",
    "\n",
    "**Q:** Po čemu se razlikuju dobivene težine i je li ta razlika očekivana? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "phi = poly.fit_transform(X)\n",
    "\n",
    "w = np.linalg.inv(phi.T @ phi + 0 * np.eye(4, 4)) @ phi.T @ y\n",
    "print(w)\n",
    "\n",
    "w = np.linalg.inv(phi.T @ phi + 1 * np.eye(4, 4)) @ phi.T @ y\n",
    "print(w)\n",
    "\n",
    "w = np.linalg.inv(phi.T @ phi + 10 * np.eye(4, 4)) @ phi.T @ y\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model), koja implementira L2-regularizirani regresijski model. Parametar $\\alpha$ odgovara parametru $\\lambda$. Primijenite model na istim primjerima kao u prethodnom zadatku i ispišite težine $\\mathbf{w}$ (atributi `coef_` i `intercept_`). Ponovno, pripazite na pomak.\n",
    "\n",
    "**Q:** Jesu li težine identične onima iz zadatka 4a? Ako nisu, objasnite zašto je to tako i kako biste to popravili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#print(phi)\n",
    "\n",
    "model = Ridge(0)\n",
    "model.fit(phi, y)\n",
    "print(f'Coef i intercept za lambda 0: {model.coef_}, {model.intercept_}')\n",
    "\n",
    "model = Ridge(1)\n",
    "model.fit(phi, y)\n",
    "print(f'Coef i intercept za lambda 1: {model.coef_}, {model.intercept_}')\n",
    "\n",
    "model = Ridge(10)\n",
    "model.fit(phi, y)\n",
    "print(f'Coef i intercept za lambda 10: {model.coef_}, {model.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Vratimo se na slučaj $N=50$ slučajno generiranih primjera iz zadatka 2. Trenirajte modele polinomijalne regresije $\\mathcal{H}_{\\lambda,d}$ za $\\lambda\\in\\{0,100\\}$ i $d\\in\\{2,10\\}$ (ukupno četiri modela). Skicirajte pripadne funkcije $h(\\mathbf{x})$ i primjere (na jednom grafikonu; preporučujemo koristiti `plot` unutar `for` petlje).\n",
    "\n",
    "**Q:** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 100]\n",
    "degrees = [2, 10]\n",
    "\n",
    "X = make_instances(-5, 5, 50)\n",
    "y = make_labels(X, lambda x: 5 + x - 2 * x**2 - 5 * x**3, 200)\n",
    "\n",
    "for i in lambdas:\n",
    "\n",
    "    for d in degrees:\n",
    "\n",
    "        polynomial_matrix = PolynomialFeatures(d).fit_transform(X)\n",
    "        model = Ridge(alpha=i)\n",
    "        model.fit(polynomial_matrix, y)\n",
    "\n",
    "        predict = model.predict(polynomial_matrix)\n",
    "        plt.plot(X, predict)\n",
    "\n",
    "        print(f\"Gubitak sa lambda {i} i stupnjem {d}: {mean_squared_error(y, predict)}\")\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Kao u zadataku 3b, razdvojite primjere na skup za učenje i skup za ispitivanje u omjeru 1:1. Prikažite krivulje logaritama pogreške učenja i ispitne pogreške u ovisnosti za model $\\mathcal{H}_{d=10,\\lambda}$, podešavajući faktor regularizacije $\\lambda$ u rasponu $\\lambda\\in\\{0,1,\\dots,50\\}$.\n",
    "\n",
    "**Q:** Kojoj strani na grafikonu odgovara područje prenaučenosti, a kojoj podnaučenosti? Zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednosti za $\\lambda$ izabrali na temelju ovih grafikona i zašto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = make_instances(-5, 5, 50)\n",
    "y = make_labels(X, lambda x: 5 + x - 2 * x**2 - 5 * x**3, 200)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=83)\n",
    "stupanj = 10\n",
    "lambdas = [x for x in range(1001)]\n",
    "poly = PolynomialFeatures(stupanj)\n",
    "\n",
    "error_train = []\n",
    "error_test = []\n",
    "\n",
    "for l in lambdas:\n",
    "\n",
    "    poly_matrix_train = poly.fit_transform(X_train)\n",
    "    model = Ridge(l)\n",
    "    model.fit(poly_matrix_train, y_train)\n",
    "\n",
    "    predict_train = model.predict(poly_matrix_train)\n",
    "    error_train.append(mean_squared_error(y_train, predict_train))\n",
    "\n",
    "    poly_matrix_test = poly.fit_transform(X_test)\n",
    "    predict_test = model.predict(poly_matrix_test)\n",
    "    error_test.append(mean_squared_error(y_test, predict_test))\n",
    "\n",
    "plt.plot(np.array(lambdas).reshape(-1, 1), np.log(np.array(error_train).reshape(-1, 1)))\n",
    "plt.plot(np.array(lambdas).reshape(-1, 1), np.log(np.array(error_test).reshape(-1, 1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatni zadatci\n",
    "\n",
    "Zadatci u nastavku (označeni zvjezdicom) nisu dio obaveznog dijela laboratorijske vježbe, niti nose bonus bodove. Dakle, nije ih potrebno riješiti kako biste ostvarili 100% bodova na ovoj laboratorijskoj vježbi. Međutim, preporučamo vam da ih pokušate riješiti i na terminu predaje prodiskutirate svoja rješenja s asistentima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *5. L1-regularizacija i L2-regularizacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svrha regularizacije jest potiskivanje težina modela $\\mathbf{w}$ prema nuli, kako bi model bio što jednostavniji. Složenost modela može se okarakterizirati normom pripadnog vektora težina $\\mathbf{w}$, i to tipično L2-normom ili L1-normom. Za jednom trenirani model možemo izračunati i broj ne-nul značajki, ili L0-normu, pomoću sljedeće funkcije koja prima vektor težina $\\mathbf{w}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonzeroes(coef, tol=1e-6): \n",
    "    return len(coef) - len(coef[np.isclose(0, coef, atol=tol)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Za ovaj zadatak upotrijebite skup za učenje i skup za testiranje iz zadatka 3b. Trenirajte modele **L2-regularizirane** polinomijalne regresije stupnja $d=5$, mijenjajući hiperparametar $\\lambda$ u rasponu $\\{1,2,\\dots,100\\}$. Za svaki od treniranih modela izračunajte L{0,1,2}-norme vektora težina $\\mathbf{w}$ te ih prikažite kao funkciju od $\\lambda$. Pripazite što točno šaljete u funkciju za izračun normi.\n",
    "\n",
    "**Q:** Objasnite oblik obiju krivulja. Hoće li krivulja za $\\|\\mathbf{w}\\|_2$ doseći nulu? Zašto? Je li to problem? Zašto?\n",
    "\n",
    "**Q:** Za $\\lambda=100$, koliki je postotak težina modela jednak nuli, odnosno koliko je model rijedak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def l1_norm(x):\n",
    "    x = np.abs(x)\n",
    "    return np.sum(x)\n",
    "\n",
    "def l2_norm(x):\n",
    "    x = x ** 2\n",
    "    x = np.sum(x)\n",
    "    return np.sqrt(x)\n",
    "\n",
    "X = make_instances(-5, 5, 50)\n",
    "y = make_labels(X, lambda x: 5 + x - 2 * x**2 - 5 * x**3, 200)\n",
    "poly = PolynomialFeatures(5)\n",
    "X_poly = poly.fit_transform(X)\n",
    "lambdas = [x for x in range(1, 101)]\n",
    "\n",
    "l_0, l_1, l_2 = [], [], []\n",
    "\n",
    "brojac = 0\n",
    "\n",
    "for l in lambdas:\n",
    "\n",
    "    model = Ridge(l)\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    if brojac % 10 == 0:\n",
    "        print(model.coef_)\n",
    "\n",
    "    l_0.append(nonzeroes(model.coef_))\n",
    "    l_1.append(l1_norm(model.coef_))\n",
    "    l_2.append(l2_norm(model.coef_))\n",
    "\n",
    "    brojac += 1\n",
    "\n",
    "plt.plot(np.array(lambdas), np.array(l_0))\n",
    "print(f'L1: {l_1}')\n",
    "print(f'L2: {l_2}')\n",
    "plt.plot(np.array(lambdas), np.array(l_1))\n",
    "plt.plot(np.array(lambdas), np.array(l_2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavna prednost L1-regularizirane regresije (ili *LASSO regression*) nad L2-regulariziranom regresijom jest u tome što L1-regularizirana regresija rezultira **rijetkim modelima** (engl. *sparse models*), odnosno modelima kod kojih su mnoge težine pritegnute na nulu. Pokažite da je to doista tako, ponovivši gornji eksperiment s **L1-regulariziranom** regresijom, implementiranom u klasi  [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) u modulu [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = make_instances(-5, 5, 50)\n",
    "y = make_labels(X, lambda x: 5 + x - 2 * x**2 - 5 * x**3, 200)\n",
    "poly = PolynomialFeatures(5)\n",
    "X_poly = poly.fit_transform(X)\n",
    "lambdas = [x for x in range(1, 101)]\n",
    "\n",
    "l_0, l_1, l_2 = [], [], []\n",
    "\n",
    "brojac = 0\n",
    "for l in lambdas:\n",
    "\n",
    "    model = Lasso(l)\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    if brojac % 10 == 0:\n",
    "        print(model.coef_)\n",
    "\n",
    "    l_0.append(nonzeroes(model.coef_))\n",
    "    l_1.append(l1_norm(model.coef_))\n",
    "    l_2.append(l2_norm(model.coef_))\n",
    "\n",
    "    brojac += 1\n",
    "\n",
    "plt.plot(np.array(lambdas), np.array(l_0))\n",
    "print(f'L1: {l_1}')\n",
    "print(f'L2: {l_2}')\n",
    "plt.plot(np.array(lambdas), np.array(l_1))\n",
    "plt.plot(np.array(lambdas), np.array(l_2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *6. Značajke različitih skala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Često se u praksi možemo susreti sa podatcima u kojima sve značajke nisu jednakih magnituda. Primjer jednog takvog skupa je regresijski skup podataka `grades` u kojem se predviđa prosjek ocjena studenta na studiju (1--5) na temelju dvije značajke: bodova na prijamnom ispitu (1--3000) i prosjeka ocjena u srednjoj školi. Prosjek ocjena na studiju izračunat je kao težinska suma ove dvije značajke uz dodani šum.\n",
    "\n",
    "Koristite sljedeći kôd kako biste generirali ovaj skup podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data_points = 500\n",
    "np.random.seed(69)\n",
    "\n",
    "# Generiraj podatke o bodovima na prijamnom ispitu koristeći normalnu razdiobu i ograniči ih na interval [1, 3000].\n",
    "exam_score = np.random.normal(loc=1500.0, scale = 500.0, size = n_data_points) \n",
    "exam_score = np.round(exam_score)\n",
    "exam_score[exam_score > 3000] = 3000\n",
    "exam_score[exam_score < 0] = 0\n",
    "\n",
    "# Generiraj podatke o ocjenama iz srednje škole koristeći normalnu razdiobu i ograniči ih na interval [1, 5].\n",
    "grade_in_highschool = np.random.normal(loc=3, scale = 2.0, size = n_data_points)\n",
    "grade_in_highschool[grade_in_highschool > 5] = 5\n",
    "grade_in_highschool[grade_in_highschool < 1] = 1\n",
    "\n",
    "# Matrica dizajna.\n",
    "grades_X = np.array([exam_score,grade_in_highschool]).T\n",
    "\n",
    "# Završno, generiraj izlazne vrijednosti.\n",
    "rand_noise = np.random.normal(loc=0.0, scale = 0.5, size = n_data_points)\n",
    "exam_influence = 0.9\n",
    "grades_y = ((exam_score / 3000.0) * (exam_influence) + (grade_in_highschool / 5.0) \\\n",
    "            * (1.0 - exam_influence)) * 5.0 + rand_noise\n",
    "grades_y[grades_y < 1] = 1\n",
    "grades_y[grades_y > 5] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iscrtajte ovisnost ciljne vrijednosti (y-os) o prvoj i o drugoj značajki (x-os). Iscrtajte dva odvojena grafa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].scatter(exam_score, grades_y)\n",
    "axarr[0].set_title('Prijamni')\n",
    "axarr[1].scatter(grade_in_highschool, grades_y)\n",
    "axarr[1].set_title('Prosjek ocjena')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naučite model L2-regularizirane regresije ($\\lambda = 0.01$), na podacima `grades_X` i `grades_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.01)\n",
    "model.fit(grades_X, grades_y)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ponovite gornji eksperiment, ali prvo skalirajte podatke `grades_X` i `grades_y` i spremite ih u varijable `grades_X_fixed` i `grades_y_fixed`. Za tu svrhu, koristite [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "grades_X_fixed = scaler.fit_transform(grades_X)\n",
    "grades_y_fixed = scaler.fit_transform(grades_y.reshape(-1, 1))\n",
    "\n",
    "model = Ridge(alpha=0.01)\n",
    "model.fit(grades_X_fixed, grades_y_fixed)\n",
    "print(model.coef_)\n",
    "\n",
    "\"\"\"\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].scatter(grades_X_fixed.T[0], grades_y_fixed)\n",
    "axarr[0].set_title('Prijamni')\n",
    "axarr[1].scatter(grades_X_fixed.T[1], grades_y_fixed)\n",
    "axarr[1].set_title('Prosjek ocjena')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Gledajući grafikone iz podzadatka (a), koja značajka bi trebala imati veću magnitudu, odnosno važnost pri predikciji prosjeka na studiju? Odgovaraju li težine Vašoj intuiciji? Objasnite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *7. Multikolinearnost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izradite skup podataka `grades_X_fixed_colinear` tako što ćete u skupu `grades_X_fixed` iz\n",
    "zadatka *7b* duplicirati zadnji stupac (ocjenu iz srednje škole). Time smo efektivno uveli savršenu multikolinearnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grades_X_fixed_colinear = np.append(grades_X_fixed, grades_X_fixed.T[1].reshape(-1, 1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponovno, naučite na ovom skupu L2-regularizirani model regresije ($\\lambda = 0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.01)\n",
    "model.fit(grades_X_fixed_colinear, grades_y)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Usporedite iznose težina s onima koje ste dobili u zadatku *7b*. Što se dogodilo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slučajno uzorkujte 50% elemenata iz skupa `grades_X_fixed_colinear` i naučite dva modela L2-regularizirane regresije, jedan s $\\lambda=0.01$ i jedan s $\\lambda=1000$). Ponovite ovaj pokus 10 puta (svaki put s drugim podskupom od 50% elemenata).  Za svaki model, ispišite dobiveni vektor težina u svih 10 ponavljanja te ispišite standardnu devijaciju vrijednosti svake od težina (ukupno šest standardnih devijacija, svaka dobivena nad 10 vrijednosti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Ridge(alpha=0.01)\n",
    "model2 = Ridge(alpha=1000)\n",
    "koef1 = []\n",
    "koef2 = []\n",
    "\n",
    "for i in range(10):\n",
    "    indices = np.random.choice(grades_X_fixed_colinear.shape[0], 250, replace=False)\n",
    "    model1.fit(grades_X_fixed_colinear[indices], grades_y[indices])\n",
    "    model2.fit(grades_X_fixed_colinear[indices], grades_y[indices])\n",
    "\n",
    "    print(f'Model 1: {model1.coef_}')\n",
    "    print(f'Model 2: {model2.coef_}')\n",
    "\n",
    "    koef1.append(model1.coef_)\n",
    "    koef2.append(model2.coef_)\n",
    "\n",
    "koef1 = np.array(koef1)\n",
    "koef2 = np.array(koef2)\n",
    "\n",
    "for i in koef1.T:\n",
    "    print(f'Model 1 std: {np.std(i)}')\n",
    "\n",
    "for i in koef2.T:\n",
    "    print(f'Model 2 std: {np.std(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na stabilnost težina?  \n",
    "**Q:** Jesu li koeficijenti jednakih magnituda kao u prethodnom pokusu? Objasnite zašto."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
